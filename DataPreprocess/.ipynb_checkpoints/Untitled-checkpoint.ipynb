{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('./../..')\n",
    "sys.path.append('./..')\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import multiprocessing as mp\n",
    "from joblib import Parallel, delayed\n",
    "import argparse\n",
    "import re\n",
    "import yaml\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()\n",
    "\n",
    "CONFIG = None\n",
    "DIR_LOC = None\n",
    "CONFIG = None\n",
    "CONFIG_FILE = 'config.yaml'\n",
    "ID_COL = 'PanjivaRecordID'\n",
    "categorical_columns = None\n",
    "use_cols = None\n",
    "freq_bound = None\n",
    "save_dir = None\n",
    "categorical_columns = None\n",
    "numeric_columns = None\n",
    "\n",
    "VALID_HSCODE_LIST = []\n",
    "with open('./valid_HSCodes.txt','r') as fh:\n",
    "    VALID_HSCODE_LIST = fh.readlines()\n",
    "VALID_HSCODE_LIST = [_.strip('\\n') for _ in VALID_HSCODE_LIST]\n",
    "\n",
    "\n",
    "\n",
    "def HSCode_filter_aux(val):\n",
    "    global VALID_HSCODE_LIST\n",
    "    val = str(val)\n",
    "    vals = val.split(';')\n",
    "\n",
    "    for _val in vals:\n",
    "        _val = str(_val)\n",
    "        _val = _val.replace('.', '')\n",
    "        _val = str(_val[:8])\n",
    "        if _val[:2] == '44':\n",
    "            return _val\n",
    "        elif _val in VALID_HSCODE_LIST:\n",
    "            return _val\n",
    "        else:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def set_up_config(_DIR=None):\n",
    "    global DIR\n",
    "    global CONFIG\n",
    "    global CONFIG_FILE\n",
    "    global use_cols\n",
    "    global num_neg_samples_ape\n",
    "    global save_dir\n",
    "    global column_value_filters\n",
    "    global num_neg_samples\n",
    "    global DATA_SOURCE\n",
    "    global DIR_LOC\n",
    "    global freq_bound_PERCENTILE\n",
    "    global freq_bound_ABSOLUTE\n",
    "    global ID_COL\n",
    "    global numeric_columns\n",
    "    global categorical_columns\n",
    "    DATA_SOURCE = './../Data_Raw/'\n",
    "    with open(CONFIG_FILE) as f:\n",
    "        CONFIG = yaml.safe_load(f)\n",
    "\n",
    "    if _DIR is not None:\n",
    "        DIR = _DIR\n",
    "        CONFIG['DIR'] = _DIR\n",
    "    else:\n",
    "        DIR = CONFIG['DIR']\n",
    "    numeric_columns = list(sorted(CONFIG['numeric_columns']))\n",
    "    categorical_columns = list(sorted(CONFIG['categorical_columns']))\n",
    "    ID_COL = 'PanjivaRecordID'\n",
    "    DIR_LOC = re.sub('[0-9]', '', DIR)\n",
    "#     DATA_SOURCE = os.path.join(DATA_SOURCE, DIR_LOC)\n",
    "    save_dir = CONFIG['save_dir']\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "    save_dir = os.path.join(\n",
    "        CONFIG['save_dir'],\n",
    "        DIR\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "\n",
    "    use_cols = [ID_COL] + categorical_columns +  numeric_columns\n",
    "    freq_bound_PERCENTILE = CONFIG['freq_bound_PERCENTILE']\n",
    "    freq_bound_ABSOLUTE = CONFIG['freq_bound_ABSOLUTE']\n",
    "    column_value_filters = CONFIG[DIR]['column_value_filters']\n",
    "\n",
    "    _cols = list(use_cols)\n",
    "    _cols.remove(ID_COL)\n",
    "    attribute_columns = categorical_columns +  numeric_columns    \n",
    "    return\n",
    "\n",
    "\n",
    "def clean_Quantity(val):\n",
    "    if val is None :\n",
    "        return None\n",
    "    if type(val) ==str and len(val) == 0:\n",
    "        return None\n",
    "    try:\n",
    "        res = int(val.split(' ')[0]) \n",
    "    except:\n",
    "        res = None\n",
    "    return res\n",
    "\n",
    "def get_regex(_type):\n",
    "    global DIR\n",
    "    if DIR == 'us_import1':\n",
    "        if _type == 'train':\n",
    "            return '.*0[1-2]_2015.csv'\n",
    "        if _type == 'test':\n",
    "            return '.*0[3]_2015.csv'\n",
    "\n",
    "    if DIR == 'us_import2':\n",
    "        if _type == 'train':\n",
    "            return '.*0[4-5]_2015.csv'\n",
    "        if _type == 'test':\n",
    "            return '.*0[6]_2015.csv'\n",
    "\n",
    "    if DIR == 'us_import3':\n",
    "        if _type == 'train':\n",
    "            return '.*0[2-3]_2016.csv'\n",
    "        if _type == 'test':\n",
    "            return '.*0[4]_2016.csv'\n",
    "        \n",
    "    if DIR == 'us_import4':\n",
    "        if _type == 'train':\n",
    "            return '.*0[4-4]_2016.csv'\n",
    "        if _type == 'test':\n",
    "            return '.*0[6]_2016.csv'\n",
    "    \n",
    "    if DIR == 'us_import5':\n",
    "        if _type == 'train':\n",
    "            return '.*0[6-7]_2017.csv'\n",
    "        if _type == 'test':\n",
    "            return '.*0[8]_2017.csv'\n",
    "    \n",
    "    if DIR == 'us_import6':\n",
    "        if _type == 'train':\n",
    "            return '.*0[5-6]_2015.csv'\n",
    "        if _type == 'test':\n",
    "            return '.*0[7]_2015.csv'\n",
    "    return '*.csv'\n",
    "\n",
    "\n",
    "def get_files(DIR, _type='all'):\n",
    "    global DATA_SOURCE\n",
    "    data_dir = DATA_SOURCE\n",
    "\n",
    "    regex = get_regex(_type)\n",
    "    print(regex)\n",
    "    c = glob.glob(os.path.join(data_dir, '*'))\n",
    "\n",
    "    def glob_re(pattern, strings):\n",
    "        return filter(re.compile(pattern).match, strings)\n",
    "\n",
    "    files = sorted([_ for _ in glob_re(regex, c)])\n",
    "\n",
    "    print('DIR ::', DIR, ' Type ::', _type, 'Files count::', len(files))\n",
    "    return files\n",
    "\n",
    "def remove_low_frequency_values(df):\n",
    "    global id_col\n",
    "    global freq_bound_PERCENTILE\n",
    "    global freq_bound_ABSOLUTE\n",
    "    global categorical_columns\n",
    "\n",
    "    freq_column_value_filters = {}\n",
    "    feature_cols = list(categorical_columns)\n",
    "    print('feature columns ::', feature_cols)\n",
    "    # ----\n",
    "    # Figure out which entities are to be removed\n",
    "    # ----\n",
    "\n",
    "    counter_df = pd.DataFrame(columns=['domain', 'count'])\n",
    "    for c in feature_cols:\n",
    "        count = len(set(df[c]))\n",
    "        counter_df = counter_df.append({\n",
    "            'domain': c, 'count': count\n",
    "        }, ignore_index=True)\n",
    "\n",
    "        z = np.percentile(\n",
    "            list(Counter(df[c]).values()), 5)\n",
    "       \n",
    "\n",
    "    counter_df = counter_df.sort_values(by=['count'], ascending=True)\n",
    "    print(' Data frame of Number of values', counter_df)\n",
    "\n",
    "    for c in list(counter_df['domain']):\n",
    "\n",
    "        values = list(df[c])\n",
    "        freq_column_value_filters[c] = []\n",
    "        obj_counter = Counter(values)\n",
    "        for _item, _count in obj_counter.items():\n",
    "            if _count < freq_bound_PERCENTILE or _count < freq_bound_ABSOLUTE:\n",
    "                freq_column_value_filters[c].append(_item)\n",
    "\n",
    "    print('Removing :: ')\n",
    "    for c, _items in freq_column_value_filters.items():\n",
    "        print('column : ', c, 'count', len(_items))\n",
    "\n",
    "    print(' DF length : ', len(df))\n",
    "    for col, val in freq_column_value_filters.items():\n",
    "        df = df.loc[~df[col].isin(val)]\n",
    "\n",
    "    print(' DF length : ', len(df))\n",
    "    return df\n",
    "\n",
    "\n",
    "def apply_value_filters(list_df):\n",
    "    global column_value_filters\n",
    "\n",
    "    if type(column_value_filters) != bool:\n",
    "        list_processed_df = []\n",
    "        for df in list_df:\n",
    "            for col, val in column_value_filters.items():\n",
    "                df = df.loc[~df[col].isin(val)]\n",
    "            list_processed_df.append(df)\n",
    "        return list_processed_df\n",
    "    return list_df\n",
    "\n",
    "\n",
    "\n",
    "def attribute_cleanup(list_df):\n",
    "    new_list = []\n",
    "    for _df in list_df:\n",
    "        _df['HSCode'] = _df['HSCode'].parallel_apply(HSCode_filter_aux)\n",
    "        _df = _df.dropna(subset=['HSCode'])\n",
    "        _df['Quantity'] = _df['Quantity'].parallel_apply(clean_Quantity) \n",
    "        _df = _df.dropna(subset=['Quantity'])\n",
    "        print(' In HSCode clean up , length of dataframe ', len(_df))\n",
    "        new_list.append(_df)\n",
    "    return new_list\n",
    "\n",
    "def clean_train_data():\n",
    "    global DIR\n",
    "    global CONFIG\n",
    "    global DIR_LOC\n",
    "    global categorical_columns\n",
    "    global numeric_columns\n",
    "    \n",
    "    files = get_files(DIR, 'train')\n",
    "    print('Columns read ', use_cols)\n",
    "    list_df = [\n",
    "        pd.read_csv(_file, usecols=use_cols, low_memory=False) for _file in files\n",
    "    ]\n",
    "    list_df = [_.dropna() for _ in list_df]\n",
    "    list_df = attribute_cleanup(list_df)\n",
    "    list_df_1 = apply_value_filters(list_df)\n",
    "    master_df = None\n",
    "\n",
    "    for df in list_df_1:\n",
    "        if master_df is None:\n",
    "            master_df = pd.DataFrame(df, copy=True)\n",
    "        else:\n",
    "            master_df = master_df.append(\n",
    "                df,\n",
    "                ignore_index=True\n",
    "            )\n",
    "    master_df = remove_low_frequency_values(master_df)\n",
    "    print(len(master_df))\n",
    "    return master_df\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_ids(\n",
    "        df,\n",
    "        save_dir\n",
    "):\n",
    "    global id_col\n",
    "    global freq_bound\n",
    "    global categorical_columns, numeric_columns\n",
    "    pandarallel.initialize()\n",
    "\n",
    "    feature_columns = categorical_columns\n",
    "    dict_DomainDims = {}\n",
    "    col_val2id_dict = {}\n",
    "\n",
    "    for col in feature_columns:\n",
    "        vals = list(set(df[col]))\n",
    "        vals = list(sorted(vals))\n",
    "\n",
    "        id2val_dict = {\n",
    "            e[0]: e[1]\n",
    "            for e in enumerate(vals, 0)\n",
    "        }\n",
    "        print(' > ', col, ':', len(id2val_dict))\n",
    "\n",
    "        val2id_dict = {\n",
    "            v: k for k, v in id2val_dict.items()\n",
    "        }\n",
    "        col_val2id_dict[col] = val2id_dict\n",
    "\n",
    "        # Replace\n",
    "        df[col] = df.parallel_apply(\n",
    "            replace_attr_with_id,\n",
    "            axis=1,\n",
    "            args=(col, val2id_dict,)\n",
    "        )\n",
    "\n",
    "        dict_DomainDims[col] = len(id2val_dict)\n",
    "\n",
    "    print(' dict_DomainDims ', dict_DomainDims)\n",
    "\n",
    "    # -------------\n",
    "    # Save the domain dimensions\n",
    "    # -------------\n",
    "\n",
    "    file = 'domain_dims.pkl'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "    f_path = os.path.join(save_dir, file)\n",
    "\n",
    "    with open(f_path, 'wb') as fh:\n",
    "        pickle.dump(\n",
    "            dict_DomainDims,\n",
    "            fh,\n",
    "            pickle.HIGHEST_PROTOCOL\n",
    "        )\n",
    "\n",
    "    file = 'col_val2id_dict.pkl'\n",
    "    f_path = os.path.join(save_dir, file)\n",
    "\n",
    "    with open(f_path, 'wb') as fh:\n",
    "        pickle.dump(\n",
    "            col_val2id_dict,\n",
    "            fh,\n",
    "            pickle.HIGHEST_PROTOCOL\n",
    "        )\n",
    "    return df, col_val2id_dict\n",
    "\n",
    "def replace_attr_with_id(row, attr, val2id_dict):\n",
    "    val = row[attr]\n",
    "    if val not in val2id_dict.keys():\n",
    "        return None\n",
    "    else:\n",
    "        return val2id_dict[val]\n",
    "\n",
    "def order_cols(df):\n",
    "    global categorical_columns\n",
    "    global numeric_columns\n",
    "    global ID_COL\n",
    "\n",
    "    ord_cols = [ID_COL] + categorical_columns + numeric_columns\n",
    "    return df[ord_cols]\n",
    "\n",
    "def setup_testing_data(\n",
    "        test_df,\n",
    "        train_df,\n",
    "        col_val2id_dict\n",
    "):\n",
    "    global id_col\n",
    "    global save_dir\n",
    "    global categorical_columns, numeric_columns\n",
    "    test_df = test_df.dropna()\n",
    "\n",
    "    # Replace with None if ids are not in train_set\n",
    "    feature_cols = list(categorical_columns)\n",
    "\n",
    "    for col in feature_cols:\n",
    "        valid_items = list(col_val2id_dict[col].keys())\n",
    "        test_df = test_df.loc[test_df[col].isin(valid_items)]\n",
    "\n",
    "    # First convert to to ids\n",
    "    for col in feature_cols:\n",
    "        val2id_dict = col_val2id_dict[col]\n",
    "        test_df[col] = test_df.parallel_apply(\n",
    "            replace_attr_with_id,\n",
    "            axis=1,\n",
    "            args=(\n",
    "                col,\n",
    "                val2id_dict,\n",
    "            )\n",
    "        )\n",
    "    test_df = test_df.dropna()\n",
    "    test_df = test_df.drop_duplicates(subset= categorical_columns + numeric_columns)\n",
    "    \n",
    "    test_df = order_cols(test_df)\n",
    "\n",
    "    print(' Length of testing data', len(test_df))\n",
    "    test_df = order_cols(test_df)\n",
    "    return test_df\n",
    "\n",
    "def create_train_test_sets():\n",
    "    global use_cols\n",
    "    global DIR\n",
    "    global save_dir\n",
    "    global column_value_filters\n",
    "    global CONFIG\n",
    "    global DIR_LOC\n",
    "    global categorical_columns,numeric_columns\n",
    "    \n",
    "    train_df_file = os.path.join(save_dir, 'train_data.csv')\n",
    "    test_df_file = os.path.join(save_dir, 'test_data.csv')\n",
    "    column_valuesId_dict_file = 'column_valuesId_dict.pkl'\n",
    "    column_valuesId_dict_path = os.path.join(\n",
    "        save_dir,\n",
    "        column_valuesId_dict_file\n",
    "    )\n",
    "\n",
    "    # --- Later on - remove using the saved file ---- #\n",
    "\n",
    "    if os.path.exists(train_df_file) and os.path.exists(test_df_file):\n",
    "        train_df = pd.read_csv(train_df_file)\n",
    "        test_df = pd.read_csv(test_df_file)\n",
    "        with open(column_valuesId_dict_path, 'rb') as fh:\n",
    "            col_val2id_dict = pickle.load(fh)\n",
    "\n",
    "        return train_df, test_df, col_val2id_dict\n",
    "\n",
    "    train_df = clean_train_data()\n",
    "    train_df = order_cols(train_df)\n",
    "    train_df, col_val2id_dict = convert_to_ids(\n",
    "        train_df,\n",
    "        save_dir\n",
    "    )\n",
    "    \n",
    "    train_df = train_df.drop_duplicates(subset=categorical_columns + numeric_columns)\n",
    "   \n",
    "    print('Length of train data ', len(train_df))\n",
    "    train_df = order_cols(train_df)\n",
    "\n",
    "    '''\n",
    "         test data preprocessing\n",
    "    '''\n",
    "    # combine test data into 1 file :\n",
    "    test_files = get_files(DIR, 'test')\n",
    "    list_test_df = [\n",
    "        pd.read_csv(_file, low_memory=False, usecols=use_cols)\n",
    "        for _file in test_files\n",
    "    ]\n",
    "    list_test_df = [_.dropna() for _ in list_test_df]\n",
    "    list_test_df = attribute_cleanup(list_test_df)\n",
    "\n",
    "    test_df = None\n",
    "    for _df in list_test_df:\n",
    "        if test_df is None:\n",
    "            test_df = _df\n",
    "        else:\n",
    "            test_df = test_df.append(_df)\n",
    "\n",
    "    print('size of  Test set ', len(test_df))\n",
    "    test_df = setup_testing_data(\n",
    "        test_df,\n",
    "        train_df,\n",
    "        col_val2id_dict\n",
    "    )\n",
    "\n",
    "    test_df.to_csv(test_df_file, index=False)\n",
    "    train_df.to_csv(train_df_file, index=False)\n",
    "\n",
    "    # Save data_dimensions.csv ('column', dimension')\n",
    "    dim_df = pd.DataFrame(columns=['column', 'dimension'])\n",
    "    for col in categorical_columns:\n",
    "        _count = len(col_val2id_dict[col])\n",
    "        dim_df = dim_df.append({'column': col, 'dimension': _count}, ignore_index=True)\n",
    "\n",
    "    dim_df.to_csv(os.path.join(save_dir, 'data_dimensions.csv'), index=False)\n",
    "\n",
    "    # -----------------------\n",
    "    # Save col_val2id_dict\n",
    "    # -----------------------\n",
    "    with open(column_valuesId_dict_path, 'wb') as fh:\n",
    "        pickle.dump(col_val2id_dict, fh, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return train_df, test_df, col_val2id_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--DIR', choices=['us_import1', 'us_import2', 'us_import3', 'us_import4', 'us_import5' , 'us_import6'],\n",
    "    default= None\n",
    ")\n",
    "\n",
    "args = parser.parse_args()\n",
    "DIR = args.DIR\n",
    "# -------------------------------- #\n",
    "set_up_config(args.DIR)\n",
    "create_train_test_sets()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Carrier</th>\n",
       "      <th>ShipmentOrigin</th>\n",
       "      <th>ShipmentDestination</th>\n",
       "      <th>PortOfUnlading</th>\n",
       "      <th>PortOfLading</th>\n",
       "      <th>VolumeTEU</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>WeightKg</th>\n",
       "      <th>NumberOfContainers</th>\n",
       "      <th>HSCode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMAW - Apex Shipping Co</td>\n",
       "      <td>China</td>\n",
       "      <td>Port of Long Beach, Long Beach, California</td>\n",
       "      <td>Port of Long Beach, Long Beach, California</td>\n",
       "      <td>Tianjin, China</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1008 CTN</td>\n",
       "      <td>11296.0</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OSTI - Orient Star Transport Intl Ltd</td>\n",
       "      <td>China</td>\n",
       "      <td>The Port of Los Angeles, Los Angeles, California</td>\n",
       "      <td>The Port of Los Angeles, Los Angeles, California</td>\n",
       "      <td>Yantian, China</td>\n",
       "      <td>1.00</td>\n",
       "      <td>93 CTN</td>\n",
       "      <td>2871.0</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EXDO - Expeditors International Of Washington Inc</td>\n",
       "      <td>China</td>\n",
       "      <td>Port of Portland, Portland, Oregon</td>\n",
       "      <td>Port of Portland, Portland, Oregon</td>\n",
       "      <td>Busan, South Korea</td>\n",
       "      <td>2.00</td>\n",
       "      <td>60 CAS</td>\n",
       "      <td>34473.0</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CGLT - Casia Global Logistics Co Ltd</td>\n",
       "      <td>China</td>\n",
       "      <td>Port of Tacoma, Tacoma, Washington</td>\n",
       "      <td>Port of Tacoma, Tacoma, Washington</td>\n",
       "      <td>Yantian, China</td>\n",
       "      <td>2.00</td>\n",
       "      <td>411 CTN</td>\n",
       "      <td>7432.0</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AEIG - American International Cargo Service Inc</td>\n",
       "      <td>China</td>\n",
       "      <td>New York/Newark Area, Newark, New Jersey</td>\n",
       "      <td>New York/Newark Area, Newark, New Jersey</td>\n",
       "      <td>Shanghai, China</td>\n",
       "      <td>0.68</td>\n",
       "      <td>406 CTN</td>\n",
       "      <td>5366.0</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796820</th>\n",
       "      <td>PNEP - Pantainer Express Line Panalpina Inc As...</td>\n",
       "      <td>Germany</td>\n",
       "      <td>New York/Newark Area, Newark, New Jersey</td>\n",
       "      <td>New York/Newark Area, Newark, New Jersey</td>\n",
       "      <td>Hamburg, Germany</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1 PKG</td>\n",
       "      <td>113.0</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796821</th>\n",
       "      <td>EXDO - Expeditors International Of Washington Inc</td>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>The Port of Los Angeles, Los Angeles, California</td>\n",
       "      <td>The Port of Los Angeles, Los Angeles, California</td>\n",
       "      <td>Hong Kong, Hong Kong</td>\n",
       "      <td>0.02</td>\n",
       "      <td>12 CTN</td>\n",
       "      <td>188.0</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796822</th>\n",
       "      <td>EXDO - Expeditors International Of Washington Inc</td>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>Service Port-San Francisco, San Francisco, Cal...</td>\n",
       "      <td>The Port of Los Angeles, Los Angeles, California</td>\n",
       "      <td>Hong Kong, Hong Kong</td>\n",
       "      <td>0.03</td>\n",
       "      <td>25 CTN</td>\n",
       "      <td>251.0</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796823</th>\n",
       "      <td>PNEP - Pantainer Express Line Panalpina Inc As...</td>\n",
       "      <td>Germany</td>\n",
       "      <td>New York/Newark Area, Newark, New Jersey</td>\n",
       "      <td>New York/Newark Area, Newark, New Jersey</td>\n",
       "      <td>Hamburg, Germany</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3 PKG</td>\n",
       "      <td>382.0</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796824</th>\n",
       "      <td>APLU - Apl Co Pte Ltd Nol Group</td>\n",
       "      <td>Guatemala</td>\n",
       "      <td>The Port of Los Angeles, Los Angeles, California</td>\n",
       "      <td>The Port of Los Angeles, Los Angeles, California</td>\n",
       "      <td>Lazaro Cardenas, Mexico</td>\n",
       "      <td>2.00</td>\n",
       "      <td>665 CTN</td>\n",
       "      <td>5494.0</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>796825 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Carrier ShipmentOrigin  \\\n",
       "0                                 AMAW - Apex Shipping Co          China   \n",
       "1                   OSTI - Orient Star Transport Intl Ltd          China   \n",
       "2       EXDO - Expeditors International Of Washington Inc          China   \n",
       "3                    CGLT - Casia Global Logistics Co Ltd          China   \n",
       "4         AEIG - American International Cargo Service Inc          China   \n",
       "...                                                   ...            ...   \n",
       "796820  PNEP - Pantainer Express Line Panalpina Inc As...        Germany   \n",
       "796821  EXDO - Expeditors International Of Washington Inc      Hong Kong   \n",
       "796822  EXDO - Expeditors International Of Washington Inc      Hong Kong   \n",
       "796823  PNEP - Pantainer Express Line Panalpina Inc As...        Germany   \n",
       "796824                    APLU - Apl Co Pte Ltd Nol Group      Guatemala   \n",
       "\n",
       "                                      ShipmentDestination  \\\n",
       "0              Port of Long Beach, Long Beach, California   \n",
       "1        The Port of Los Angeles, Los Angeles, California   \n",
       "2                      Port of Portland, Portland, Oregon   \n",
       "3                      Port of Tacoma, Tacoma, Washington   \n",
       "4                New York/Newark Area, Newark, New Jersey   \n",
       "...                                                   ...   \n",
       "796820           New York/Newark Area, Newark, New Jersey   \n",
       "796821   The Port of Los Angeles, Los Angeles, California   \n",
       "796822  Service Port-San Francisco, San Francisco, Cal...   \n",
       "796823           New York/Newark Area, Newark, New Jersey   \n",
       "796824   The Port of Los Angeles, Los Angeles, California   \n",
       "\n",
       "                                          PortOfUnlading  \\\n",
       "0             Port of Long Beach, Long Beach, California   \n",
       "1       The Port of Los Angeles, Los Angeles, California   \n",
       "2                     Port of Portland, Portland, Oregon   \n",
       "3                     Port of Tacoma, Tacoma, Washington   \n",
       "4               New York/Newark Area, Newark, New Jersey   \n",
       "...                                                  ...   \n",
       "796820          New York/Newark Area, Newark, New Jersey   \n",
       "796821  The Port of Los Angeles, Los Angeles, California   \n",
       "796822  The Port of Los Angeles, Los Angeles, California   \n",
       "796823          New York/Newark Area, Newark, New Jersey   \n",
       "796824  The Port of Los Angeles, Los Angeles, California   \n",
       "\n",
       "                   PortOfLading  VolumeTEU  Quantity  WeightKg  \\\n",
       "0                Tianjin, China       1.00  1008 CTN   11296.0   \n",
       "1                Yantian, China       1.00    93 CTN    2871.0   \n",
       "2            Busan, South Korea       2.00    60 CAS   34473.0   \n",
       "3                Yantian, China       2.00   411 CTN    7432.0   \n",
       "4               Shanghai, China       0.68   406 CTN    5366.0   \n",
       "...                         ...        ...       ...       ...   \n",
       "796820         Hamburg, Germany       0.01     1 PKG     113.0   \n",
       "796821     Hong Kong, Hong Kong       0.02    12 CTN     188.0   \n",
       "796822     Hong Kong, Hong Kong       0.03    25 CTN     251.0   \n",
       "796823         Hamburg, Germany       0.03     3 PKG     382.0   \n",
       "796824  Lazaro Cardenas, Mexico       2.00   665 CTN    5494.0   \n",
       "\n",
       "        NumberOfContainers HSCode  \n",
       "0                        1   None  \n",
       "1                        1   None  \n",
       "2                        2   None  \n",
       "3                        1   None  \n",
       "4                        1   None  \n",
       "...                    ...    ...  \n",
       "796820                   1   None  \n",
       "796821                   1   None  \n",
       "796822                   1   None  \n",
       "796823                   1   None  \n",
       "796824                   1   None  \n",
       "\n",
       "[796825 rows x 10 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
